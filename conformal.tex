\section{Conformal Prediction}
\begin{definition}
	Conformal prediction is a method of making predictions that is designed to provide a measure of confidence or uncertainty in the prediction. It is based on the idea of creating a prediction region, or "conformal prediction set," around a new data point, which contains a range of possible values for the prediction. The prediction is considered "valid" if the true value of the data point falls within this region.
	The mathematical formula for conformal prediction is as follows:
	\begin{displaymath}
		P(\text{true label} \in \text{prediction set}) \geq 1 - \alpha

	\end{displaymath}
	Here, $P(\text{true label} \in \text{prediction set})$ is the probability that the true label of the data point falls within the prediction set, and $\alpha$ is a user-specified confidence level, typically set to a small value such as 0.1 or 0.05.

	Conformal prediction is used to provide a measure of uncertainty in predictions, which can be useful in situations where it is important to know how confident we are in a prediction. For example, in medical diagnosis, it may be important to know how certain we are that a patient has a particular disease, in order to make informed treatment decisions. In such cases, conformal prediction can be used to provide a range of possible diagnoses, along with a corresponding confidence level.

	Conformal prediction is also used in situations where the data is noisy or uncertain, as it can provide a way to account for this uncertainty in the predictions. This is especially useful when working with complex or high-dimensional data, where the relationship between the features and the target variable may be difficult to model accurately.
\end{definition}
\paragraph{Properties}
\begin{itemize}
	\item Guaranteed validity - with respect to probability of error
	\item Commonplace in stats - Confidence Intervals, prediction intervals.
\end{itemize}

\subsection{Explanation of Algo}
The high level idea behind conformal prediction is to create a prediction set or region, around a new data point that contains a rage of possible values for the prediction. This prediction set is constructed using the conformity scores and the p\_value of the data points of the training and test values

\begin{itemize}
	\item Given a training set of data points $z_1, z_2, ..., z_n$ and a new test sample $x*$, we first compute the conformity scores, or p-values, for each possible label $y$ for the test sample.
	\item The conformity score for a particular label $y$ is calculated as the number of training data points with conformity scores less than or equal to the conformity score of the test sample, divided by the total number of training data points plus one. This is represented by the formula:
	      $$ p(y) = \frac{#{i = 1, ..., n+1 | \alpha_y^i \leq \alpha_y^{n+1}}}{n+1} $$

	      where $\alpha_y^1, ..., \alpha_y^n, \alpha_y^{n+1}$ are the conformity scores corresponding to $z_1, ..., z_n, (x*, y)$.
\end{itemize}

\begin{itemize}
	\item Given a significance level $\epsilon > 0$ (our target probability of error), we can then compute the corresponding prediction set $\Gamma_\epsilon = {y \in Y | p(y) > \epsilon}$. This is the set of labels that are considered "valid" predictions for the test sample, as they have a probability of error less than or equal to $\epsilon$.
	\item To make a prediction for the test sample, we can choose the label with the highest conformity score, or we can select the label with the highest conformity score that falls within the prediction set $\Gamma_\epsilon$. This allows us to balance the trade-off between accuracy and confidence in the prediction.
\end{itemize}
Overall, the goal of conformal prediction is to provide a measure of confidence or uncertainty in the prediction, by constructing a prediction set that contains a range of possible values for the prediction, rather than a single point estimate. This can be useful in situations where it is important to know how confident we are in a prediction, or where the data is noisy or uncertain.

\subsubsection{Special Cases}
Most cases it is important to understand how strange our data is. This refers to how different a data point is from the rest of the training data. A dta point with a high degree of strangeness is one that is significantly different from the other data points in the training set.

The special cases are of the following: \italic{Because the slides are shit}
\begin{itemize}
	\item If the test sample has the highest conformity score. Of all the data points in the training set. This means that it is the most strange data point. In this case the conformity score of the test sample will be $ \frac{1}{n+1} $ Which is the smallest possible value.
	\item If the test sample has the second highest conformity score of all the data points in the training set, this means that it is the second most strange data. In this case it will have a conformity score of the test
	      $\frac{2}{n+1}$
	\item If the test sample has the lowest conformity score of all the data points in the training set, this means that it is the most "conforming" data point. In this case, the conformity score of the test sample will be 1.

\end{itemize}
These special cases illustrate how the conformity score of the test sample is influenced by its relationship to the conformity scores of the training data points. In general, the higher the conformity score of the test sample, the more "strange" it is relative to the training data, and the lower the conformity score, the more "conforming" it is. This is important to consider when constructing the prediction set, as the conformity scores of the test sample and the training data points are used to determine which labels are considered "valid" predictions for the test sample.

\subsubsection{Assumptions towards Conformity}
The assumptions of machine learning refer to the assumptions that are made about the data and the learning process when building and using machine learning models. These assumptions can vary depending on the type of model and the specific problem being solved, but they generally include assumptions about the structure of the data, the relationship between the features and the target variable, and the nature of the learning process itself.

Conformal prediction is a method of making predictions that is designed to provide a measure of confidence or uncertainty in the prediction. It is based on the idea of creating a prediction region, or "conformal prediction set," around a new data point, which contains a range of possible values for the prediction. The prediction is considered "valid" if the true value of the data point falls within this region.

Conformal prediction based on nearest neighbors is a specific approach to conformal prediction that uses the k-nearest neighbors (KNN) algorithm to find the nearest neighbors of a new data point and use their labels to make a prediction. The conformity scores of the data points are calculated using the KNN distances, and the prediction set is constructed based on these conformity scores.

The validity and efficiency of conformal predictors refer to the properties of the prediction set that is constructed around a new data point. Validity refers to the probability that the true label of the data point falls within the prediction set, which is typically set to a user-specified value such as 0.1 or 0.05. Efficiency refers to the size of the prediction set, with smaller prediction sets being considered more efficient.

The validity of conformal predictors is ensured by the property that the predictor makes a mistake with probability at most $\epsilon$, provided that the labeled samples are independently and identically distributed (IID). This means that the probability of the true label falling outside of the prediction set is at most $\epsilon$.

The efficiency of conformal predictors can be traded off against their validity. In general, smaller prediction sets are more efficient, but they may also be less accurate, as they may not contain a wide enough range of possible values for the prediction. On the other hand, larger prediction sets may be more accurate, but they may also be less efficient. It is important to consider both validity and efficiency when using conformal prediction, in order to find the optimal balance between these two properties.


\subsection{Conformity Measure for KNN (K=1)}
There are several different conformity measures that can be used with the 1-nearest neighbor (1-NN) algorithm for conformal prediction. Here is an explanation of two of these measures:

The distance to the nearest sample of a different class: This conformity measure is based on the idea that a data point that is more similar to data points from a different class is more likely to be classified correctly. In other words, if the nearest neighbor of a data point belongs to a different class, this may be a good indication that the data point is correctly classified.

The conformity score for this measure can be calculated as the distance between the data point and its nearest neighbor of a different class. For example, if the data point is classified as class A and its nearest neighbor is classified as class B, the conformity score would be the distance between the two points.

This conformity measure can be represented mathematically as follows:



One over the distance to the nearest sample of the same class:
This conformity measure is defined as follows:
$$ \alpha = \frac{1}{d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{Same})} $$
where $\mathbf{x}$ is the test sample, $\mathbf{x}_\text{NN}^\text{Same}$ is the nearest sample of the same class as $\mathbf{x}$, and $d(\cdot, \cdot)$ is a distance function that measures the distance between two samples. This conformity measure assigns a higher score to test samples that are closer to their nearest same-class sample, and a lower score to test samples that are farther away.

The distance to the nearest sample of a different class divided by the distance to the nearest sample of the same class:
This conformity measure is defined as follows:
$$ \alpha = \frac{d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{diff})}{d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{Same})} $$
where $\mathbf{x}$ is the test sample, $\mathbf{x}_\text{NN}^\text{diff}$ is the nearest sample of a different class as $\mathbf{x}$, and $\mathbf{x}_\text{NN}^\text{same}$ is the nearest sample of the same class as $\mathbf{x}$. This conformity measure assigns a higher score to test samples that are closer to their nearest different-class sample and farther away from their nearest same-class sample, and a lower score to test samples that are farther away from their nearest different-class sample and closer to their nearest same-class sample.








Both of these conformity measures are based on the distance between the data point and its nearest neighbors, with the first measure using the distance to the nearest neighbor of a different class and the second measure using the distance to the nearest neighbor of the same class.

The first conformity measure, which uses the distance to the nearest neighbor of a different class, may be more suitable in situations where the classes are well-separated and the data points from different classes are significantly different from one another. In such cases, the distance to the nearest neighbor of a different class may provide a good indication of the confidence of the prediction.

On the other hand, the second conformity measure, which uses the distance to the nearest neighbor of the same class, may be more suitable in situations where the classes are more overlapping and the data points from different classes are more similar to one another. In such cases, the distance to the nearest neighbor of the same class may provide a better indication of the confidence of the prediction.

\subsection{Ranking Algo}
\textit{It is a stupid algo, as simple as it May look, for some reason it can become quite jaring to figure out. This should debunk majority of its bullshit}

\begin{definition}
	A ranking algorithm is a method for ordering a list of items according to some criterion. The criterion could be based on a variety of factors, such as relevance, popularity, or quality.
	We will use this for the p\_value, to rank items that will have the same probability
\end{definition}

\subsubsection{Example}
Imagine we have a list of students and their grades on a test, and we want to rank the students by their grades. The list looks like this:
\begin{center}
	\begin{tabular}{ |c| c| }
		\hline
		Student & Grade \\
		\hline
		Alice   & 90    \\
		Bob     & 80    \\
		Charlie & 90    \\
		Dave    & 70    \\
		Eve     & 80    \\
		Frank   & 60    \\

		\hline
	\end{tabular}
\end{center}

To deal with duplicate values, we can first sort the list in descending order of grades. This will group the students with the same grade together:

\begin{center}
	\begin{tabular}{ |c|c| }

		\hline
		Student & Grade \\
		\hline
		Alice   & 90    \\
		Charlie & 90    \\
		Bob     & 80    \\
		Eve     & 80    \\
		Dave    & 70    \\
		Frank   & 60    \\
		\hline
	\end{tabular}
\end{center}
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Rank & Student & Grade \\
		\hline
		1    & Alice   & 90    \\
		2    & Charlie & 90    \\
		3    & Bob     & 80    \\
		3    & Eve     & 80    \\
		4    & Dave    & 70    \\
		5    & Frank   & 60    \\
		\hline
	\end{tabular}
\end{center}

\begin{itemize}
	\item Sort the list in descending order of grades.
	\item Initialize the rank to 1.
	\item For each student in the list:
	      \begin{itemize}
		      \item If the student has a higher grade than the previous student, increment the rank by 1.
		      \item If the student has the same grade as the previous student, do not increment the rank.
		      \item Assign the rank to the student.
	      \end{itemize}
	\item return Ranked List
\end{itemize}

\subsection{Example}
Take our original data Where we want to calculate the following\\
\begin{itemize}
	\item P\_value
	\item Conformity Score
\end{itemize}
\end{center}
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Sample  & Label \\
		\hline
		(0,3)   & +1    \\
		(2,2)   & +1    \\
		(3,3)   & +1    \\
		(-1,1)  & -1    \\
		(-1,-1) & -1    \\
		(0,1)   & -1    \\
		\hline
	\end{tabular}
\end{center}


For example, consider a dataset with two labels, 0 and 1, and a feature set containing two features, $x_1$ and $x_2$. Where $x_1 = 0 $ and $x_2 = 0$.

\paragraph{Steps}
To calculate the conformity scores and p-values for the conformity measure "the distance to the nearest sample of a different class divided by the distance to the nearest sample of the same class," you can follow these steps:

\begin{itemize}
	\item For each sample in the training set, calculate the conformity score for the given conformity measure. To do this, you will need to compute the distance to the nearest sample of a different class and the distance to the nearest sample of the same class. For example, for the positive sample $(0, 3)$, the conformity score would be calculated as $\frac{2}{\sqrt{4} + 1} \approx 0.894$, where 2 is the distance to the nearest negative sample $(-1, 1)$ and $\sqrt{4} + 1$ is the distance to the nearest positive sample $(2, 2)$.
	\item Repeat the calculation for the test sample, using the label you are interested in. For example, if you are interested in the p-value for the positive label, you would calculate the conformity score for the test sample $(0, 0)$ with the label +1. The conformity score for the test sample with the positive label would be calculated as $\frac{1}{\sqrt{4} + 4} \approx 0.354$, where 1 is the distance to the nearest negative sample $(-1, -1)$ and $\sqrt{4} + 4$ is the distance to the nearest positive sample $(3, 3)$.
	\item Sort the conformity scores of the training samples, including the conformity score of the test sample, in ascending order.
	\item Count the number of conformity scores that are less than or equal to the conformity score of the test sample.
	\item Divide the number of conformity scores that are less than or equal to the conformity score of the test sample by the total number of conformity scores plus one, to get the p-value.
\end{itemize}


\paragraph{Calcuation}


\begin{center}
	\begin{tabular}{ccc}
		\hline
		Sample   & Label & Conformity score                                  \\
		\hline
		(0, 3)   & +1    & $\frac{2}{\sqrt{4} + 1} \approx 0.894$            \\
		(2, 2)   & +1    & $\frac{\sqrt{4} + 1}{\sqrt{1} + 1} \approx 1.581$ \\
		(3, 3)   & +1    & $\frac{\sqrt{9} + 4}{\sqrt{1} + 1} \approx 2.550$ \\
		(-1, 1)  & -1    & $\frac{\sqrt{1} + 1}{1} \approx 1.414$            \\
		(-1, -1) & -1    & $\frac{\sqrt{1} + 1}{2} \approx 0.707$            \\
		(0, 1)   & -1    & $\frac{1}{1} = 1$                                 \\
		(0, 0)   & +1    & $(?) \frac{1}{\sqrt{4} + 4} \approx 0.354$        \\
		\hline
	\end{tabular}
\end{center}
Graphical Representation :\\
\hspace*{-0.6cm}\begin{tikzpicture}
	\begin{axis}[
			xlabel={$x$},
			ylabel={Conformity score},
			xmin=-2.5, xmax=4,
			ymin=0, ymax=3,
		]
		\addplot[
			red,
			mark=x,
			mark size=4pt,
			only marks,
		] coordinates {
				(0,0.894)
			};
		\addplot[
			red,
			mark=x,
			mark size=4pt,
			only marks,
		] coordinates {
				(2,1.581)
			};
		\addplot[
			red,
			mark=x,
			mark size=4pt,
			only marks,
		] coordinates {
				(3,2.550)
			};
		\addplot[
			blue,
			mark=square,
			mark size=4pt,
			only marks,
		] coordinates {
				(-1,1.414)
			};
		\addplot[
			blue,
			mark=square,
			mark size=4pt,
			only marks,
		] coordinates {
				(-1,0.707)
			};
		\addplot[
			blue,
			mark=square,
			mark size=4pt,
			only marks,
		] coordinates {
				(0,1)
			};
		\draw[red] (0,0.894) -- (0,0);
		\draw[red] (2,1.581) -- (2,0);
		\draw[red] (3,2.550) -- (3,0);
		\draw[blue] (-1,1.414) -- (-1,0);
		\draw[blue] (-1,0.707) -- (-1,0);
		\draw[blue] (0,1) -- (0,0);
	\end{axis}
\end{tikzpicture}
\begin{tabular}{c c c}
	\hline
	Sample   & Label & Conformity score difference \\ [0.5ex]
	\hline
	(0, 3)   & +1    & 0.894                       \\
	(2, 2)   & +1    & 1.581                       \\
	(3, 3)   & +1    & 2.550                       \\ [1ex]
	\hline
	(−1, 1)  & −1    & 1.414                       \\
	(−1, −1) & −1    & 0.707                       \\
	(0, 1)   & −1    & 1                           \\ [1ex]
	\hline
\end{tabular}
The conformity score of a sample can also be represented using the following equation:

$$ \alpha = d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{diff}) $$

In this equation, $\alpha$ represents the conformity score of the sample, $\mathbf{x}$ represents the coordinates of the sample, and $d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{diff})$ represents the distance between the sample and its nearest neighbor of a different class. The nearest neighbor of a different class is represented by $\mathbf{x}_\text{NN}^\text{diff}$.

For example, to calculate the conformity score of the first positive sample (0, 3), you can use the following equation:

$$ \alpha = d((0, 3), (-1, -1)) $$

This equation calculates the distance between the first positive sample (0, 3) and its nearest neighbor of a different class (-1, -1). The distance can be calculated using the Euclidean distance formula:

$$ d((0, 3), (-1, -1)) = \sqrt{(-1 - 0)^2 + (-1 - 3)^2} = 2 $$

Substituting this value into the conformity score equation gives:

$$ \alpha = \frac{2}{\sqrt{4} + 1} = 0.894 $$

This calculation can be repeated for each sample in the dataset to obtain the conformity scores for all samples.

\paragraph{Prediction and how does ranking work ? }
To calculate the p-value using the ranking method, you need to rank all conformity scores in ascending order. The p-value for a particular label is then calculated by dividing the rank of the conformity score for that label by the total number of samples in the dataset, including the test sample.

For example, to calculate the p-value for the label +1 in the given dataset, you can first rank all conformity scores in ascending order as follows:

0.707, 0.894, 1.414, 1.581, 2.550, 0.354

The conformity score for the label +1 (0.354) is the sixth smallest conformity score. Therefore, the p-value for the label +1 is calculated as follows:

$$ p(\text{+1}) = \frac{6}{7} = 0.857 $$

Similarly, the p-value for the label -1 is calculated as follows:

$$ p(\text{-1}) = \frac{1}{7} = 0.143 $$

The p-values for all labels can be calculated in this way, and the prediction set can then be determined by comparing the p-values to the significance level $\epsilon$. If a p-value is greater than $\epsilon$, the corresponding label is included in the prediction set. For example, if the significance level is set to $\epsilon = 0.1$, then the prediction set for the given dataset would be $\Gamma_{0.1} = {\text{-1}}$, since the p-value for the label -1 (0.143) is greater than $\epsilon$.

\paragraph{Ranking}
In the ranking method, the conformity score for the test sample is compared to the conformity scores for all other samples in the dataset. If the conformity score for the test sample is the smallest (or largest), it gets a rank of 1 (or the total number of samples). If the conformity score for the test sample is the second smallest (or second largest), it gets a rank of 2 (or the total number of samples minus 1). This process continues until the conformity score for the test sample is ranked.

In the given dataset, the conformity score for the test sample (0.354) is the sixth smallest conformity score. Therefore, it gets a rank of 6. The total number of samples in the dataset, including the test sample, is 7. Therefore, the p-value for the label +1 is calculated as follows:

$$ p(\text{+1}) = \frac{6}{7} = 0.857 $$

Similarly, the conformity score for the label -1 (0.707) is the smallest conformity score and gets a rank of 1. The p-value for the label -1 is calculated as follows:
$$ p(\text{-1}) = \frac{1}{7} = 0.143 $$
This is why the p-values for the labels +1 and -1 are 6/7 and 1/7, respectively.


\subsection{Things that I wish I had known about}
\begin{itemize}
	\item You rank through lowest to highest with respect to the item, that you are labelling.
	\item You do not need to skip numbers, just label them based on what they are.
	      1 2 3 4 5
	\item On labeling, mark your target value, and $\alpha = x/n $
	\item your confidence is mostly $ (1-\alpha) $
\end{itemize}


\newpage
\section{Non Conformity}
\begin{definition}[Non Conformitu]
	Nonconformity measures are defined in the same way as conformity measures, but with a different interpretation: $\alpha_i$ measures how strange (rather than how conforming) $z_i$ is. The formula for computing p-values becomes:

	$$ p(y) = \frac{#{i = 1, ..., n+1 | \alpha_y^i \geq \alpha_y^{n+1}}}{n+1} $$

	Note that the only difference between this formula and the one for conformity measures is that the inequality symbol has been flipped from $\leq$ to $\geq$.

	Instead of using nonconformity scores $\alpha_i$, one can also use conformity scores $-\alpha_i$ (or $\frac{1}{\alpha_i}$ if $\alpha_i$ are positive). However, in the context of regression, nonconformity scores are often more convenient.
\end{definition}



A common way to measure nonconformity is to use a conformity measure, but with the opposite interpretation. In other words, instead of measuring how similar a sample is to the training data, nonconformity measures how dissimilar or abnormal a sample is.

Formally, nonconformity can be defined as follows:

$$ \alpha = \max\left{0, d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{same}) - d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{diff})\right} $$

where $d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{same})$ is the distance from the sample $\mathbf{x}$ to its nearest neighbor of the same class, and $d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{diff})$ is the distance from the sample $\mathbf{x}$ to its nearest neighbor of a different class.

This equation ensures that nonconformity is always non-negative and that a sample is considered abnormal if its nearest neighbor of the same class is farther away than its nearest neighbor of a different class.

Nw consider the following example :
\begin{center}
	\begin{tabular}{ c c c }
		positive & Negative \\
		\hline
		0        & 11       \\
		\hline
		1        & 10
	\end{tabular}
\end{center}
Let the training set be ${(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}$ and the test sample be $(x_{n+1}, y_{n+1})$. The conformity measure for the $i$-th sample is denoted by $\alpha_i$.

For the postulated label $y_{n+1} = +1$, the p-value is given by

$$ p(+1) = \frac{#{i \mid 1 \leq i \leq n+1, \alpha_i \geq \alpha_{n+1}}}{n+1} = \frac{1}{5} = 0.2 $$

For the postulated label $y_{n+1} = -1$, the p-value is given by

$$ p(-1) = \frac{#{i \mid 1 \leq i \leq n+1, \alpha_i \geq \alpha_{n+1}}}{n+1} = \frac{5}{5} = 1 $$

The average false p-value is 0.5 (computed as $(0.2 + 1)/2$).

Our test value will always have priority when sorting the items .
One way to represent this mathematically is to use a function $p(x)$ that returns the priority of a given element $x$. For example, in the given set $x$, we can define the function as follows:

$$p(x) = \begin{cases} 100 & \text{if } x = 1:100 \ 50 & \text{otherwise} \end{cases}$$

This function assigns a priority of 100 to the element $x = 1:100$, and a priority of 50 to all other elements.

To sort the elements in the set $x$ according to their priority, we can use a sorting algorithm that takes the priority into account. For example, we can use a sorting algorithm that sorts the elements in ascending order of their priority. In this case, the element with a priority of 100 would be placed first, followed by the elements with a priority of 50.


\subsection{Two Forms of Non conformity}
The first nonconformity measure is simply the absolute difference between the actual value, $y_i$, and the predicted value, $\hat{y}_i$. This measure is simple to compute and understand, and it provides a straightforward way to gauge the strangeness of a given sample.

The second nonconformity measure adds an additional term, $\sigma_i$, which represents an estimate of the accuracy of the prediction. This measure is useful because it takes into account the uncertainty of the prediction, which can be particularly important in cases where the predicted values have a high degree of variability. The size of the prediction set is more adaptive because the measure adjusts to the uncertainty of the prediction, which means that it is more sensitive to samples that are farther away from the predicted value.

Mathematically, these nonconformity measures can be expressed as follows:

$$ \alpha_i = \left| y_i - \hat{y}_i \right| $$

$$ \alpha_i = \frac{\left| y_i - \hat{y}_i \right|}{\sigma_i} $$

In both cases, the larger the value of $\alpha_i$, the more strange the sample is considered to be.

\subsection{Validty and efficiency of conformal predictions}
The probability that $y^* \not\in \Gamma_{\frac{1}{n+1}}$ can be represented mathematically as:

$$P(y^* \not\in \Gamma_{\frac{1}{n+1}})$$

This probability is equal to the probability that $z^* = (x^, y^)$ is the strangest labeled sample in the set $z_1, ..., z_n, z^*$. This can be expressed as:

$$P(z^* = \text{strangest labeled sample in } z_1, ..., z_n, z^*)$$

By the IID (Independent and Identically Distributed) assumption, all permutations of $z_1, ..., z_n, z^*$ (and the corresponding permutations of $\alpha_1, ..., \alpha_n, \alpha_{n+1}$) have the same probability. Therefore, the probability that the smallest conformity score in the bag ${\alpha_1, ..., \alpha_{n+1}}$ will be the last one is exactly $\frac{1}{n+1}$ (provided there is only one smallest element in the bag; otherwise the probability of error will be less, 0). This can be represented mathematically as:

$$P(\text{smallest conformity score in } {\alpha_1, ..., \alpha_{n+1}} \text{ is the last one}) = \frac{1}{n+1}$$
