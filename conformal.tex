\section{Conformal Prediction}
\begin{definition}
	Conformal prediction is a method of making predictions that is designed to provide a measure of confidence or uncertainty in the prediction. It is based on the idea of creating a prediction region, or "conformal prediction set," around a new data point, which contains a range of possible values for the prediction. The prediction is considered "valid" if the true value of the data point falls within this region.

	The mathematical formula for conformal prediction is as follows:
	\begin{displaymath}
		P(\text{true label} \in \text{prediction set}) \geq 1 - \alpha

	\end{displaymath}
	Here, $P(\text{true label} \in \text{prediction set})$ is the probability that the true label of the data point falls within the prediction set, and $\alpha$ is a user-specified confidence level, typically set to a small value such as 0.1 or 0.05.

	Conformal prediction is used to provide a measure of uncertainty in predictions, which can be useful in situations where it is important to know how confident we are in a prediction. For example, in medical diagnosis, it may be important to know how certain we are that a patient has a particular disease, in order to make informed treatment decisions. In such cases, conformal prediction can be used to provide a range of possible diagnoses, along with a corresponding confidence level.

	Conformal prediction is also used in situations where the data is noisy or uncertain, as it can provide a way to account for this uncertainty in the predictions. This is especially useful when working with complex or high-dimensional data, where the relationship between the features and the target variable may be difficult to model accurately.
\end{definition}
\paragraph{Properties}
\begin{itemize}
	\item Guaranteed validity - with respect to probability of error
	\item Commonplace in stats - Confidence Intervals, prediction intervals.
\end{itemize}

\subsection{Explanation of Algo}
The high level idea behind conformal prediction is to create a prediction set or region, around a new data point that contains a rage of possible values for the prediction. This prediction set is constructed using the conformity scores and the p\_value of the data points of the training and test values

\begin{itemize}
	\item Given a training set of data points $z_1, z_2, ..., z_n$ and a new test sample $x*$, we first compute the conformity scores, or p-values, for each possible label $y$ for the test sample.
	\item The conformity score for a particular label $y$ is calculated as the number of training data points with conformity scores less than or equal to the conformity score of the test sample, divided by the total number of training data points plus one. This is represented by the formula:
	      $$ p(y) = \frac{#{i = 1, ..., n+1 | \alpha_y^i \leq \alpha_y^{n+1}}}{n+1} $$

	      where $\alpha_y^1, ..., \alpha_y^n, \alpha_y^{n+1}$ are the conformity scores corresponding to $z_1, ..., z_n, (x*, y)$.
\end{itemize}

\begin{itemize}
	\item Given a significance level $\epsilon > 0$ (our target probability of error), we can then compute the corresponding prediction set $\Gamma_\epsilon = {y \in Y | p(y) > \epsilon}$. This is the set of labels that are considered "valid" predictions for the test sample, as they have a probability of error less than or equal to $\epsilon$.
	\item To make a prediction for the test sample, we can choose the label with the highest conformity score, or we can select the label with the highest conformity score that falls within the prediction set $\Gamma_\epsilon$. This allows us to balance the trade-off between accuracy and confidence in the prediction.
\end{itemize}
Overall, the goal of conformal prediction is to provide a measure of confidence or uncertainty in the prediction, by constructing a prediction set that contains a range of possible values for the prediction, rather than a single point estimate. This can be useful in situations where it is important to know how confident we are in a prediction, or where the data is noisy or uncertain.

\subsubsection{Special Cases}
Most cases it is important to understand how strange our data is. This refers to how different a data point is from the rest of the training data. A dta point with a high degree of strangeness is one that is significantly different from the other data points in the training set.

The special cases are of the following: \italic{Because the slides are shit}
\begin{itemize}
	\item If the test sample has the highest conformity score. Of all the data points in the training set. This means that it is the most strange data point. In this case the conformity score of the test sample will be $ \frac{1}{n+1} $ Which is the smallest possible value.
	\item If the test sample has the second highest conformity score of all the data points in the training set, this means that it is the second most strange data. In this case it will have a conformity score of the test
	      $\frac{2}{n+1}$
	\item If the test sample has the lowest conformity score of all the data points in the training set, this means that it is the most "conforming" data point. In this case, the conformity score of the test sample will be 1.

\end{itemize}
These special cases illustrate how the conformity score of the test sample is influenced by its relationship to the conformity scores of the training data points. In general, the higher the conformity score of the test sample, the more "strange" it is relative to the training data, and the lower the conformity score, the more "conforming" it is. This is important to consider when constructing the prediction set, as the conformity scores of the test sample and the training data points are used to determine which labels are considered "valid" predictions for the test sample.

\subsubsection{Assumptions towards Conformity}
The assumptions of machine learning refer to the assumptions that are made about the data and the learning process when building and using machine learning models. These assumptions can vary depending on the type of model and the specific problem being solved, but they generally include assumptions about the structure of the data, the relationship between the features and the target variable, and the nature of the learning process itself.

Conformal prediction is a method of making predictions that is designed to provide a measure of confidence or uncertainty in the prediction. It is based on the idea of creating a prediction region, or "conformal prediction set," around a new data point, which contains a range of possible values for the prediction. The prediction is considered "valid" if the true value of the data point falls within this region.

Conformal prediction based on nearest neighbors is a specific approach to conformal prediction that uses the k-nearest neighbors (KNN) algorithm to find the nearest neighbors of a new data point and use their labels to make a prediction. The conformity scores of the data points are calculated using the KNN distances, and the prediction set is constructed based on these conformity scores.

The validity and efficiency of conformal predictors refer to the properties of the prediction set that is constructed around a new data point. Validity refers to the probability that the true label of the data point falls within the prediction set, which is typically set to a user-specified value such as 0.1 or 0.05. Efficiency refers to the size of the prediction set, with smaller prediction sets being considered more efficient.

The validity of conformal predictors is ensured by the property that the predictor makes a mistake with probability at most $\epsilon$, provided that the labeled samples are independently and identically distributed (IID). This means that the probability of the true label falling outside of the prediction set is at most $\epsilon$.

The efficiency of conformal predictors can be traded off against their validity. In general, smaller prediction sets are more efficient, but they may also be less accurate, as they may not contain a wide enough range of possible values for the prediction. On the other hand, larger prediction sets may be more accurate, but they may also be less efficient. It is important to consider both validity and efficiency when using conformal prediction, in order to find the optimal balance between these two properties.


\subsection{Conformity Measure for KNN (K=1)}
There are several different conformity measures that can be used with the 1-nearest neighbor (1-NN) algorithm for conformal prediction. Here is an explanation of two of these measures:

The distance to the nearest sample of a different class: This conformity measure is based on the idea that a data point that is more similar to data points from a different class is more likely to be classified correctly. In other words, if the nearest neighbor of a data point belongs to a different class, this may be a good indication that the data point is correctly classified.

The conformity score for this measure can be calculated as the distance between the data point and its nearest neighbor of a different class. For example, if the data point is classified as class A and its nearest neighbor is classified as class B, the conformity score would be the distance between the two points.

This conformity measure can be represented mathematically as follows:

$$ \alpha = d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{diff}) $$

where $\alpha$ is the conformity score, $\mathbf{x}$ is the data point, and $\mathbf{x}_\text{NN}^\text{diff}$ is the nearest neighbor of a different class.

One over the distance to the nearest sample of the same class: This conformity measure is based on the opposite idea, that a data point that is more similar to data points from the same class is more likely to be classified correctly. In other words, if the nearest neighbor of a data point belongs to the same class, this may be a good indication that the data point is correctly classified.

The conformity score for this measure can be calculated as the reciprocal of the distance between the data point and its nearest neighbor of the same class. For example, if the data point is classified as class A and its nearest neighbor is also classified as class A, the conformity score would be the reciprocal of the distance between the two points.

This conformity measure can be represented mathematically as follows:

$$ \alpha = \frac{1}{d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{same})} $$

where $\alpha$ is the conformity score, $\mathbf{x}$ is the data point, and $\mathbf{x}_\text{NN}^\text{same}$ is the nearest neighbor of the same class.

Both of these conformity measures are based on the distance between the data point and its nearest neighbors, with the first measure using the distance to the nearest neighbor of a different class and the second measure using the distance to the nearest neighbor of the same class.

The first conformity measure, which uses the distance to the nearest neighbor of a different class, may be more suitable in situations where the classes are well-separated and the data points from different classes are significantly different from one another. In such cases, the distance to the nearest neighbor of a different class may provide a good indication of the confidence of the prediction.

On the other hand, the second conformity measure, which uses the distance to the nearest neighbor of the same class, may be more suitable in situations where the classes are more overlapping and the data points from different classes are more similar to one another. In such cases, the distance to the nearest neighbor of the same class may provide a better indication of the confidence of the prediction.

\subsection{Ranking Algo}
\textit{It is a stupid algo, as simple as it May look, for some reason it can become quite jaring to figure out. This should debunk majority of its bullshit}

\begin{definition}
	A ranking algorithm is a method for ordering a list of items according to some criterion. The criterion could be based on a variety of factors, such as relevance, popularity, or quality.
	We will use this for the p\_value, to rank items that will have the same probability
\end{definition}

\subsubsection{Example}
Imagine we have a list of students and their grades on a test, and we want to rank the students by their grades. The list looks like this:
\begin{center}
	\begin{tabular}{ |c| c| }
		\hline
		Student & Grade \\
		\hline
		Alice   & 90    \\
		Bob     & 80    \\
		Charlie & 90    \\
		Dave    & 70    \\
		Eve     & 80    \\
		Frank   & 60    \\

		\hline
	\end{tabular}
\end{center}

To deal with duplicate values, we can first sort the list in descending order of grades. This will group the students with the same grade together:

\begin{center}
	\begin{tabular}{ |c|c| }

		\hline
		Student & Grade \\
		\hline
		Alice   & 90    \\
		Charlie & 90    \\
		Bob     & 80    \\
		Eve     & 80    \\
		Dave    & 70    \\
		Frank   & 60    \\
		\hline
	\end{tabular}
\end{center}
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Rank & Student & Grade \\
		\hline
		1    & Alice   & 90    \\
		2    & Charlie & 90    \\
		3    & Bob     & 80    \\
		3    & Eve     & 80    \\
		4    & Dave    & 70    \\
		5    & Frank   & 60    \\
		\hline
	\end{tabular}
\end{center}

\begin{itemize}
	\item Sort the list in descending order of grades.
	\item Initialize the rank to 1.
	\item For each student in the list:
	      \begin{itemize}
		      \item If the student has a higher grade than the previous student, increment the rank by 1.
		      \item If the student has the same grade as the previous student, do not increment the rank.
		      \item Assign the rank to the student.
	      \end{itemize}
	\item return Ranked List
\end{itemize}

\subsection{Example}
Take our original data Where we want to calculate the following\\
\begin{itemize}
	\item P\_value
	\item Conformity Score
\end{itemize}
\end{center}
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Sample  & Label \\
		\hline
		(0,3)   & +1    \\
		(2,2)   & +1    \\
		(3,3)   & +1    \\
		(-1,1)  & -1    \\
		(-1,-1) & -1    \\
		(0,1)   & -1    \\
		\hline
	\end{tabular}
\end{center}


For example, consider a dataset with two labels, 0 and 1, and a feature set containing two features, $x_1$ and $x_2$. Where $x_1 = 0 $ and $x_2 = 0$.

\paragraph{Steps}
To calculate the conformity scores and p-values for the conformity measure "the distance to the nearest sample of a different class divided by the distance to the nearest sample of the same class," you can follow these steps:

\begin{itemize}
	\item For each sample in the training set, calculate the conformity score for the given conformity measure. To do this, you will need to compute the distance to the nearest sample of a different class and the distance to the nearest sample of the same class. For example, for the positive sample $(0, 3)$, the conformity score would be calculated as $\frac{2}{\sqrt{4} + 1} \approx 0.894$, where 2 is the distance to the nearest negative sample $(-1, 1)$ and $\sqrt{4} + 1$ is the distance to the nearest positive sample $(2, 2)$.
	\item Repeat the calculation for the test sample, using the label you are interested in. For example, if you are interested in the p-value for the positive label, you would calculate the conformity score for the test sample $(0, 0)$ with the label +1. The conformity score for the test sample with the positive label would be calculated as $\frac{1}{\sqrt{4} + 4} \approx 0.354$, where 1 is the distance to the nearest negative sample $(-1, -1)$ and $\sqrt{4} + 4$ is the distance to the nearest positive sample $(3, 3)$.
	\item Sort the conformity scores of the training samples, including the conformity score of the test sample, in ascending order.
	\item Count the number of conformity scores that are less than or equal to the conformity score of the test sample.
	\item Divide the number of conformity scores that are less than or equal to the conformity score of the test sample by the total number of conformity scores plus one, to get the p-value.
\end{itemize}


\paragraph{Calcuation}


\begin{center}
	\begin{tabular}{ccc}
		\hline
		Sample   & Label & Conformity score                                  \\
		\hline
		(0, 3)   & +1    & $\frac{2}{\sqrt{4} + 1} \approx 0.894$            \\
		(2, 2)   & +1    & $\frac{\sqrt{4} + 1}{\sqrt{1} + 1} \approx 1.581$ \\
		(3, 3)   & +1    & $\frac{\sqrt{9} + 4}{\sqrt{1} + 1} \approx 2.550$ \\
		(-1, 1)  & -1    & $\frac{\sqrt{1} + 1}{1} \approx 1.414$            \\
		(-1, -1) & -1    & $\frac{\sqrt{1} + 1}{2} \approx 0.707$            \\
		(0, 1)   & -1    & $\frac{1}{1} = 1$                                 \\
		(0, 0)   & +1    & $(?) \frac{1}{\sqrt{4} + 4} \approx 0.354$        \\
		\hline
	\end{tabular}
\end{center}
Graphical Representation :\\
\hspace*{-0.6cm}\begin{tikzpicture}
	\begin{axis}[
			xlabel={$x$},
			ylabel={Conformity score},
			xmin=-2.5, xmax=4,
			ymin=0, ymax=3,
		]
		\addplot[
			red,
			mark=x,
			mark size=4pt,
			only marks,
		] coordinates {
				(0,0.894)
			};
		\addplot[
			red,
			mark=x,
			mark size=4pt,
			only marks,
		] coordinates {
				(2,1.581)
			};
		\addplot[
			red,
			mark=x,
			mark size=4pt,
			only marks,
		] coordinates {
				(3,2.550)
			};
		\addplot[
			blue,
			mark=square,
			mark size=4pt,
			only marks,
		] coordinates {
				(-1,1.414)
			};
		\addplot[
			blue,
			mark=square,
			mark size=4pt,
			only marks,
		] coordinates {
				(-1,0.707)
			};
		\addplot[
			blue,
			mark=square,
			mark size=4pt,
			only marks,
		] coordinates {
				(0,1)
			};
		\draw[red] (0,0.894) -- (0,0);
		\draw[red] (2,1.581) -- (2,0);
		\draw[red] (3,2.550) -- (3,0);
		\draw[blue] (-1,1.414) -- (-1,0);
		\draw[blue] (-1,0.707) -- (-1,0);
		\draw[blue] (0,1) -- (0,0);
	\end{axis}
\end{tikzpicture}
\begin{tabular}{c c c}
	\hline
	Sample   & Label & Conformity score difference \\ [0.5ex]
	\hline
	(0, 3)   & +1    & 0.894                       \\
	(2, 2)   & +1    & 1.581                       \\
	(3, 3)   & +1    & 2.550                       \\ [1ex]
	\hline
	(−1, 1)  & −1    & 1.414                       \\
	(−1, −1) & −1    & 0.707                       \\
	(0, 1)   & −1    & 1                           \\ [1ex]
	\hline
\end{tabular}
The conformity score of a sample can also be represented using the following equation:

$$ \alpha = d(\mathbf{x}, \mathbf{x}_\text{NN}^\text{diff}) $$

In this equation, $\alpha$ represents the conformity score of the sample, $\mathbf{x}$ represents the coordinates of the sample, and $d(\mathbf{x}, \mathbf{x}\text{NN}^\text{diff})$ represents the distance between the sample and its nearest neighbor of a different class. The nearest neighbor of a different class is represented by $\mathbf{x}\text{NN}^\text{diff}$.

For example, to calculate the conformity score of the first positive sample (0, 3), you can use the following equation:

$$ \alpha = d((0, 3), (-1, -1)) $$

This equation calculates the distance between the first positive sample (0, 3) and its nearest neighbor of a different class (-1, -1). The distance can be calculated using the Euclidean distance formula:

$$ d((0, 3), (-1, -1)) = \sqrt{(-1 - 0)^2 + (-1 - 3)^2} = 2 $$

Substituting this value into the conformity score equation gives:

$$ \alpha = \frac{2}{\sqrt{4} + 1} = 0.894 $$

This calculation can be repeated for each sample in the dataset to obtain the conformity scores for all samples.

\paragraph{Prediction and how does ranking work ? }
To calculate the p-value using the ranking method, you need to rank all conformity scores in ascending order. The p-value for a particular label is then calculated by dividing the rank of the conformity score for that label by the total number of samples in the dataset, including the test sample.

For example, to calculate the p-value for the label +1 in the given dataset, you can first rank all conformity scores in ascending order as follows:

0.707, 0.894, 1.414, 1.581, 2.550, 0.354

The conformity score for the label +1 (0.354) is the sixth smallest conformity score. Therefore, the p-value for the label +1 is calculated as follows:

$$ p(\text{+1}) = \frac{6}{7} = 0.857 $$

Similarly, the p-value for the label -1 is calculated as follows:

$$ p(\text{-1}) = \frac{1}{7} = 0.143 $$

The p-values for all labels can be calculated in this way, and the prediction set can then be determined by comparing the p-values to the significance level $\epsilon$. If a p-value is greater than $\epsilon$, the corresponding label is included in the prediction set. For example, if the significance level is set to $\epsilon = 0.1$, then the prediction set for the given dataset would be $\Gamma_{0.1} = {\text{-1}}$, since the p-value for the label -1 (0.143) is greater than $\epsilon$.

\paragraph{Ranking}
In the ranking method, the conformity score for the test sample is compared to the conformity scores for all other samples in the dataset. If the conformity score for the test sample is the smallest (or largest), it gets a rank of 1 (or the total number of samples). If the conformity score for the test sample is the second smallest (or second largest), it gets a rank of 2 (or the total number of samples minus 1). This process continues until the conformity score for the test sample is ranked.

In the given dataset, the conformity score for the test sample (0.354) is the sixth smallest conformity score. Therefore, it gets a rank of 6. The total number of samples in the dataset, including the test sample, is 7. Therefore, the p-value for the label +1 is calculated as follows:

$$ p(\text{+1}) = \frac{6}{7} = 0.857 $$

Similarly, the conformity score for the label -1 (0.707) is the smallest conformity score and gets a rank of 1. The p-value for the label -1 is calculated as follows:
$$ p(\text{-1}) = \frac{1}{7} = 0.143 $$
This is why the p-values for the labels +1 and -1 are 6/7 and 1/7, respectively.

\subsection{Using Modified competition ranking}
Modified competition ranking is a method of ranking items or individuals based on their performance in a competition. It is similar to traditional competition ranking, but it takes into account the strength of the competition.

To calculate modified competition ranking, the following steps are typically followed:
\begin{itemize}
	\item Calculate the average score of each participant in the competition.
	\item Calculate the average score of the entire competition.
	\item Calculate the difference between the average score of each participant and the average score of the competition.
	\item Rank the participants based on this difference, with the highest difference ranked first.C
\end{itemize}
For example, consider a competition with three participants who score the following points:

\begin{itemize}
	\item Participant 1: 10 points
	\item Participant 2: 20 points
	\item Participant 3: 30 points
\end{itemize}

The average score of the competition is 20 points. The differences between each participant's average score and the average score of the competition are as follows:

\begin{itemize}
	\item Participant 1: 10 - 20 = -10
	\item Participant 2: 20 - 20 = 0
	\item  Participant 3: 30 - 20 = 10
\end{itemize}
This method of ranking takes into account the strength of the competition and can be useful in situations where the competition is particularly competitive or where there is a wide range of scores. It is often used in sports, where the strength of the competition can vary significantly from one match or tournament to another.

\subsection{Modified Ranking Algorithm on New solution}
Consider the following using modified ranking to sort the conformity scores:

\section{Non Conformity}
\begin{definition}[Non Conformity]
	Nonconformity measures: Nonconformity measures are used in conjunction with conformal prediction to measure how "strange" (or different) a new data point is from the training set. This can be used to construct prediction intervals that reflect the level of uncertainty around the predicted value.
	Nonconformity measures (1): Nonconformity measures are defined in the same way as conformity measures, but their interpretation is different. While conformity measures measure how "conforming" a new data point is to the training set, nonconformity measures measure how "strange" or different it is. The formula for computing p-values using nonconformity measures is similar to the formula for conformity measures, but with the inequality sign reversed.
\end{definition}
\begin{itemize}
	\item Given a training set of data points $z_1, z_2, ..., z_n$ and a new test sample $x*$, we first compute the conformity scores, or p-values, for each possible label $y$ for the test sample.
	\item The conformity score for a particular label $y$ is calculated as the number of training data points with conformity scores less than or equal to the conformity score of the test sample, divided by the total number of training data points plus one. This is represented by the formula:
	      $$ p(y) = \frac{#{i = 1, ..., n+1 | \alpha_y^i \geq \alpha_y^{n+1}}}{n+1} $$

	      where $\alpha_y^1, ..., \alpha_y^n, \alpha_y^{n+1}$ are the conformity scores corresponding to $z_1, ..., z_n, (x*, y)$.
\end{itemize}
In conformity measures, the measure is used to determine how closely a sample conforms to the rest of the training data. This is typically used in classification tasks, where the goal is to predict the class label of a sample based on its similarity to the training data. The conformity measure is used to determine the probability that the sample belongs to a particular class.

On the other hand, nonconformity measures are used to determine how unusual or different a sample is from the rest of the training data. This is typically used in regression tasks, where the goal is to predict a continuous value based on the training data. The nonconformity measure is used to determine the probability that the sample is an outlier or an unusual sample, rather than one that conforms to the general trend of the training data.

The formula for computing p-values for nonconformity measures is similar to that for conformity measures, with the main difference being that the inequality is reversed. In the formula for nonconformity measures, p(y) is the probability that the sample belongs to class y, and the nonconformity measure is used to count the number of samples that are more unusual (have a higher nonconformity measure) than the test sample, and divide by the total number of samples in the training set. This gives the probability that the test sample belongs to class y, given its unusualness compared to the rest of the training data.
