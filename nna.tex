\section{Introduction To NN}
\begin{definition}[Nearest Neighbor Algorithm]
	The nearest neighbor algorithm is a method for classification that assigns a new data point to the class of the nearest training data point. This algorithm is based on the idea that similar data points are likely to belong to the same class.
\end{definition}

Within binary classfication problems, the label space is often taken to be 0 or 1.

\subsection{How does it work ? }
Here is a mathematical example of how the nearest neighbor algorithm works:
Suppose we have a training set of data points with known classes, represented as a matrix $X \in \mathbb{R}^{n \times m}$, where each row corresponds to a data point and each column corresponds to a feature. The class labels for the data points are stored in a vector $y \in \mathbb{R}^n$.

Now, suppose we have a new data point, represented as a row vector $x \in \mathbb{R}^m$, that we want to classify. To classify this point using the nearest neighbor algorithm, we need to find the data point in the training set that is closest to $x$. We can measure the distance between $x$ and each data point in the training set using a distance metric, such as the Euclidean distance:
$$d(x, X_i) = \sqrt{\sum_{j=1}^m (x_j - X_{i,j})^2}$$

where $X_i$ is the i-th row of the matrix $X$, and $x_j$ and $X_{i,j}$ are the j-th elements of the vectors $x$ and $X_i$, respectively.

Once we have computed the distances between $x$ and all the data points in the training set, we can find the nearest neighbor by selecting the data point with the smallest distance:

$$\hat{y} = y_i, where\ i = \operatorname{argmin}_{j=1}^n d(x, X_j)$$

In this equation, $\hat{y}$ is the predicted class for the new data point $x$, and $y_i$ is the class of the nearest neighbor.

\subsubsection{Step by step guide on how NN works}


Suppose we have the following training set of data points, where each row corresponds to a data point and each column corresponds to a feature:

$$X = \begin{bmatrix}
		1 & 1 \
		2 & 2 \
		3 & 3 \
		4 & 4
	\end{bmatrix}$$

The class labels for the data points are stored in the following vector:

$$y = \begin{bmatrix}
		0 \
		1 \
		0 \
		1
	\end{bmatrix}$$

Now, suppose we have a new data point that we want to classify, represented as the following row vector:

$$x = \begin{bmatrix}
		1.5 & 1.5
	\end{bmatrix}$$

To classify this point using the nearest neighbor algorithm, we need to find the data point in the training set that is closest to $x$. We can measure the distance between $x$ and each data point in the training set using the Euclidean distance, as shown in the previous equation.

For example, the distance between $x$ and the first data point in the training set is:
\begin{align*}
	\begin{split}
		d(x, X_1) = \sqrt{(1.5 - 1)^2 + (1.5 - 1)^2} \\
		= \sqrt{0.5^2 + 0.5^2} = 0.70715
	\end{split}
\end{align*}

We can compute the distances between $x$ and all the other data points in the training set in a similar way. Once we have computed the distances, we can find the nearest neighbor by selecting the data point with the smallest distance:

$$\hat{y} = y_i, where\ i = \operatorname{argmin}_{j=1}^n d(x, X_j)$$

In this case, the nearest neighbor is the first data point, since it has the smallest distance to $x$. Therefore, the predicted class for the new data point $x$ is $\hat{y} = y_1 = 0$.


\italic{This is transduction : No model was formed}


\subsection{KNN}
KNN - K - nearest Neighbors - is a non parametric, instance based supervised learning algorithm. It is used for both classfication and gression tasks.

The basic idea of KNN is to use the information from the K nearest Nehigbors of a given data point and make a prediction, given the current set of data that we work with.

\begin{definition}
	Mathematical definition of KNN is defined :

	\begin{displaymath}
		\hat{y}  = \frac{1}{k} \sum_{i=1}^k y_i
	\end{displaymath}
\end{definition}

For example, consider a dataset with two labels, 0 and 1, and a feature set containing two features, $x_1$ and $x_2$. Let's say we want to predict the label of a new data
